```{r, warning = FALSE}
library(ggplot2)
library(caret)
```

## Summarize Data 
```{r, warning = FALSE}
# Load the training data
trainingset <- read.csv("datatraining.txt", header = TRUE)

# Load the two sets of testing data
testset1 <- read.csv("datatest.txt", header = TRUE)
testset2 <- read.csv("datatest2.txt", header = TRUE)

# Look at the dimensions of the training data
dim(trainingset)

# Look at the class of each variable
sapply(trainingset, class)

# Convert the Occupancy variable into a factor
trainingset$Occupancy <- as.factor(trainingset$Occupancy)
testset1$Occupancy <- as.factor(testset1$Occupancy)
testset2$Occupancy <- as.factor(testset2$Occupancy)

# Strip the date and use only time
trainingset$date <- format(strptime(trainingset$date, "%Y-%m-%d %H:%M:%S"), 
                           "%H:%M:%S")
testset1$date <- format(strptime(testset1$date, "%Y-%m-%d %H:%M:%S"), "%H:%M:%S")
testset2$date <- format(strptime(testset2$date, "%Y-%m-%d %H:%M:%S"), "%H:%M:%S")

# Get a summary of the data
summary(trainingset)

# Look at the distribution of the class variable
cbind(freq = table(trainingset$Occupancy),
      percentage = prop.table(table(trainingset$Occupancy)) * 100)
```
After looking at the distribution of the class variable "Occupancy". It's skewed
in favor of '0' or not occupied. 

## Visualize Data
```{r, warning=FALSE}
# Box plots of all the numeric variables
par(mfrow=c(1,5))
for(i in 2:6) {
  boxplot(trainingset[, i], main = names(trainingset)[i])
}

# Have free scales and not limited to a certain range
scales <- list(x = list(relation = "free"), y = list(relation = "free"))

# Scatter plots of all the numeric independent variables seperated by class
featurePlot(x = trainingset[, 2:6], y = trainingset[, 7], plot = "ellipse", 
            auto.key = list(columns = 2))

# Box and Whisker plots for each attribute seperated by class
featurePlot(x = trainingset[, 2:6], y = trainingset[, 7], plot = "box", scales = scales)

# Density plots for each attribute by class value
featurePlot(x = trainingset[, 2:6], y = trainingset[, 7], plot = "density", 
            scales = scales, auto.key = list(columns = 2))
```
From the plots above, especially with the density plots the following things can be noticed:
1. Light might be the biggest factor in terms of whether a room is occupied or not
2. CO2 levels when the room is not occupied is very high and low when occupied
3. There is a overlap in the distribution of the temperature curves when the room is occupied
   v/s when it is not. Although the chances that a room is occupied is higher with higher temperature
   
## Evaluate algorithms
```{r, warning=FALSE}
# Split the data set for 10-fold cross validation, train on 9, test on 1 for all combinations
trainControl <- trainControl(method = "cv", number = 10)
metric <- "Accuracy"

# Evaluate 5 different algorithms, make sure the same seed is used
# LDA
set.seed(7)
fit.lda <- train(Occupancy~., data = trainingset, method = "lda", 
                 metric = metric, trControl = trainControl)
# CART
set.seed(7)
fit.cart <- train(Occupancy~., data = trainingset, method = "rpart", 
                  metric = metric, trControl = trainControl)
# kNN
set.seed(7)
fit.knn <- train(Occupancy~., data = trainingset, method = "knn", 
                 metric = metric, trControl = trainControl)
# SVM
set.seed(7)
fit.svm <- train(Occupancy~., data = trainingset, method = "svmRadial", 
                 metric = metric, trControl = trainControl)

# Summarize accuracy of models
results <- resamples(list(lda = fit.lda, cart = fit.cart, knn = fit.knn, svm = fit.svm))
summary(results)

# Dot plot of the results
dotplot(results)
```
From the results, CART and kNN are very close in terms of accuracy and kappa, with CART only slightly better. Let's pick CART as the final algorithm.

## Finalize algorithm
```{r, warning=FALSE}
# Summary of the algorithm
print(fit.cart)

# Estimate on the test set(1 & 2)
predictions1 <- predict(fit.cart, testset1)
confusionMatrix(predictions1, testset1$Occupancy)

predictions2 <- predict(fit.cart, testset2)
confusionMatrix(predictions2, testset2$Occupancy)
```
From running the algorithm on the test data, the accuracy is pretty good 95%+ but the Kappa drops a little compared to the Training data. 